# 1. System Design for Beginners

## 3. Design Requirements

### Design Requirements
So system is not about memorizing a bunch of facts at all. It's really about the thought process, about analyzing what improvements can be made and what we have to sacrifice to get those improvements. 

#### 1) Move Data
Moving data across the network is not so staightforward, but it's boils down to moving data. 

#### 2) Store Data
Now we already talked about how storing data in RAM is different than storing data in disk because if the computer crashes or has to restart, the data on disk is persisted, but when you restart a coomputer, the data in RAM is not guaranteed. We won't be able to save this. This is just one simple example. 

#### 3) Transform Data
For example, given a bunch of application logs data, maybe we want to aggregate all of that data and transform it to get what percent of these were succesful basically theses logs are server response logs. We also wanna know what percent of requests were failed. So we would take this data as input and transform that data to get something more useful which we actually care about. 

But when it comes to designing a large application we want to be very careful with how we design it. Because bad design choices in our application architecture is very difficult to correct later. 

How do we know what is good design? One important measurement we make about a service or application is its availability.

Availability could be represented as the total uptime of server divided by uptime + downtime = total amount of time. Let's say over a day all application was up for 23h out of the total 24h. = 96%. That means 96% of time user got response.

In system design, particularly within the context of IT services and cloud computing, SLO (Service Level Objective) and SLA (Service Level Agreement) are critical concepts for defining and managing service performance and reliability.

1) Service Level Objective (SLO)

Definition:   
An SLO is a specific, measurable characteristic of the SLA, such as availability, throughput, frequency, response time, or quality. It sets the target value or range of values for the service that is provided. SLOs are used to define the expected performance level of a service in a quantitative manner.

Key Points:

- Granularity: SLOs are more granular than SLAs and can cover various aspects of a service.
- Measurable: They must be quantifiable and measurable.
- Targets: They typically include thresholds or targets (e.g., 99.9% uptime).

Examples:   
- Availability SLO: The service will be available 99.9% of the time over a month.
- Latency SLO: 95% of API calls will respond within 200 milliseconds.

2) Service Level Agreement (SLA)   

Definition:   
An SLA is a formal contract between a service provider and a customer that outlines the expected level of service. It includes a comprehensive set of SLOs and specifies the remedies or penalties if the service levels are not met.

Key Points:
- Contractual: SLAs are legally binding agreements.
- Includes SLOs: They incorporate various SLOs to define the overall service level.
- Penalties and Remedies: They outline the consequences if the agreed-upon service levels are not met (e.g., service credits, penalties).

Examples:   
- Uptime SLA: The service will have 99.9% uptime, and if this is not met, the customer will receive a service credit.
- Support Response SLA: Support requests will be responded to within 4 hours, and failure to do so will result in a penalty.

Relationship Between SLO and SLA   
- SLOs are Building Blocks of SLAs:
SLAs are constructed using multiple SLOs, each defining a specific aspect of the service.
- SLOs Provide Specifics: While an SLA is a broader agreement, SLOs provide the detailed metrics and targets that must be met to fulfill the SLA.
- SLOs Help in Monitoring and Reporting: They allow for precise monitoring of service performance and help in assessing whether the terms of the SLA are being met.


---

Systems can have reliability, fault tolerance and redundancy. 

1) Reliability : If user makes a request to our server and our server response, that means our server available but it doens't mean that it's reliable. Reliability is the probability the system won't fail. 

2) Fault Tolerance : If one portion of our system has a fault, then the system continues to operate successfully. Our system is tolerant to failures of portions of the system.

3) Redundancy : We can also call it redundancy. If we create a server that's running the exact same code. This server is redundant = we don't need it to respond user bc it's capable of everything. These two servers are redunant to each other because we don't need both of them. But it helps to have both of them bc in the event of fault/failure, we have multiple copies. So we can still do what we need to do so our system can continue the function bc we have redundancy. 

* DDoS = Distributed denial of service attack.

-> By having redundancies in our system, we're able to have fault tolerance. And with fault tolerance we can provide higher reliability. That's the relationship.

---

Throughput
- the amount of operation we can handle over some period of time. How many requests can our server handle per second.

-> How many concurrent users could our system handle per second.

Maybe we can vertically scale the server but the downside of this is we can't infinitely scale a single server. We can scale horizontally as well but we have to balance the request between server (Load Balancer)

This increases complexity but we get redundancy. But if it's database server and if we're creating more db server, it means each database can have different data.

How many requests the database can handle -> queries per second (QPS). 

Keeping data in multiple databse in sync - complicated. 

queries / second
bytes / second

How much data we can process per second.

Ultimately throughput boils down to amount of operations or amoount of data over some period of time. 

---

Latency   
- Just some period of time.
- the amount of time it takes for that operation to complete.
- the reason we have cache is to lower the latency (reading from RAM - ms / reading from CPU cache ns)
- if we lower the latency, we can increase throughput

We can have another server and we can lower latency while having redundancy and availability / reliablilty, increase throughput

Another techinque to reduce latency is having CDN (content delivery network)